{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "from pathlib import Path\n",
    "from pathlib import PurePath\n",
    "from xml.dom import minidom\n",
    "from numpy import argmax\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import Utils.utils as util\n",
    "from Utils import SECParser\n",
    "import Utils.config as conf\n",
    "\n",
    "from Utils import MANIAC as m\n",
    "\n",
    "# Load MANIAC config\n",
    "cfg = conf.get_maniac_cfg()\n",
    "\n",
    "# Folder with MANIAC keyframes\n",
    "_FOLDER = os.getcwd() + \"/MANIAC/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    This is used to create new datasets.\n",
    "    \n",
    "    Example:\n",
    "    train_set = MANIAC_DS( \"FOLDER_TO_MANIAC_GRAPHML\")\n",
    "    \n",
    "    All settings is set inside config.py\n",
    "    except save new data and create new dataset\n",
    "\n",
    "'''\n",
    "\n",
    "# Settings for creating MANIAC dataset\n",
    "_SAVE_RAW_DATA      = False\n",
    "_CREATE_DATASET     = False\n",
    "\n",
    "\n",
    "if _CREATE_DATASET:\n",
    "    train_set = m.MANIAC_DS(_FOLDER + \"training/\")\n",
    "    val_set = m.MANIAC_DS(_FOLDER + \"validation/\")\n",
    "    test_set = m.MANIAC_DS(_FOLDER + \"test/\")\n",
    "\n",
    "\n",
    "# Save datasets into _FOLDER + \"raw/maniac_training_xw.pt\"\n",
    "# This is needed for PyTorch Geometric and DataLoaders\n",
    "if _SAVE_RAW_DATA:\n",
    "    with open(os.path.join(_FOLDER + \"raw/maniac_training_\" + str(cfg.time_window) + \"w.pt\"), 'wb') as f:\n",
    "                torch.save(train_set, f)\n",
    "\n",
    "    with open(os.path.join(_FOLDER + \"raw/maniac_validation_\" + str(cfg.time_window) + \"w.pt\"), 'wb') as df:\n",
    "                torch.save(val_set, df)\n",
    "\n",
    "    with open(os.path.join(_FOLDER + \"raw/maniac_test_\" + str(cfg.time_window) + \"w.pt\"), 'wb') as df:\n",
    "                torch.save(test_set, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    This is used to create load MANIAC dataset into DataLoader.\n",
    "    \n",
    "    Example)\n",
    "    To load processed or create preprocessed data into list.\n",
    "    train_dataset = m.ManiacIMDS(_FOLDER, \"train\")\n",
    "    \n",
    "    Creates a DataLoader of the loaded dataset.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "'''\n",
    "\n",
    "# Loading pre-processed or creates new processed pt files into FOLDER/processed/\n",
    "train_dataset = m.ManiacIMDS(_FOLDER, \"train\")\n",
    "test_ds = m.ManiacIMDS(_FOLDER, \"test\")\n",
    "valid_ds = m.ManiacIMDS(_FOLDER, \"valid\")\n",
    "\n",
    "#####################PRINT################################\n",
    "print(\"Total graphs:\\t {}\\n=========\".format(len(train_dataset)+len(test_ds)+len(valid_ds)))\n",
    "print(\"Training: \\t {}\".format(len(train_dataset)))\n",
    "print(\"Test: \\t\\t {}\".format(len(test_ds)))\n",
    "print(\"Validation: \\t {}\\n=========\".format(len(valid_ds)))\n",
    "#####################PRINT################################\n",
    "\n",
    "# Create data loaders from dataset.\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.DataLoader\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "#####################PRINT################################\n",
    "print(\"Total batchs:\\t {}\\n=========\".format(len(train_loader)+len(test_loader)+len(valid_loader)))\n",
    "print(\"Training: \\t {}\".format(len(train_loader)))\n",
    "print(\"Test: \\t\\t {}\".format(len(test_loader)))\n",
    "print(\"Validation: \\t {}\\n=========\".format(len(valid_loader)))\n",
    "#####################PRINT################################\n",
    "\n",
    "# Get maximum node for graph reconstruction\n",
    "max_num_nodes_train = max([len(i.x) for i in train_dataset])\n",
    "max_num_nodes_valid = max([len(i.x) for i in valid_ds])\n",
    "max_num_nodes_test = max([len(i.x) for i in test_ds])\n",
    "max_num_nodes = max(max_num_nodes_test, max_num_nodes_train, max_num_nodes_valid)\n",
    "\n",
    "#####################PRINT################################\n",
    "print(\"Max number of nodes found:\", max_num_nodes)\n",
    "#####################PRINT################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, ReLU, ELU\n",
    "from torch_geometric.nn import NNConv, BatchNorm\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "'''\n",
    "\n",
    "    Encoder with 2x NNConv, BN\n",
    "    \n",
    "    Outputs mu, log variance, mu, prediction\n",
    "\n",
    "'''\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # NN for NNConv features\n",
    "        nn = Sequential(Linear(len(cfg.spatial_map), 64), ReLU(), Linear(64, cfg.channels * cfg.channels * 2 ))\n",
    "        nn2 = Sequential(Linear(len(cfg.spatial_map), 64), ReLU(), Linear(64, cfg.decoder_in * cfg.channels * 2 ))\n",
    "        \n",
    "        # Encoder\n",
    "        self.lin   = torch.nn.Linear(len(cfg.objects), cfg.channels) # FCL\n",
    "        self.conv1 = NNConv(cfg.channels, cfg.channels*2, nn, aggr='mean')\n",
    "        self.bn1   = BatchNorm(cfg.channels*2)\n",
    "        self.conv2 = NNConv(cfg.channels*2, cfg.channels, nn, aggr='mean')\n",
    "        \n",
    "        # Z-representation as mu and log\n",
    "        self.mu = NNConv(cfg.channels*2, cfg.decoder_in, nn2, aggr='max')\n",
    "        self.logvar = NNConv(cfg.channels*2, cfg.decoder_in, nn2, aggr='max')\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Input\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # FCL\n",
    "        out = F.relu(self.lin(data.x))\n",
    "        \n",
    "        # Used for ACTION PREDICTION\n",
    "        hidden = F.relu(self.conv1(out, data.edge_index, data.edge_attr))       # Conv1\n",
    "        hidden = self.bn1(hidden)                                               # BatchNorm\n",
    "        conv2_out = F.relu(self.conv2(hidden, data.edge_index, data.edge_attr)) # Conv2\n",
    "        \n",
    "        p_x = scatter_mean(conv2_out, batch, dim=0)\n",
    "        \n",
    "        # Used for GRAPH RECONSTRUCTION\n",
    "        mu = self.mu(hidden, data.edge_index, data.edge_attr)\n",
    "        logvar = self.logvar(hidden, data.edge_index, data.edge_attr)\n",
    "        mu = scatter_mean(mu, batch, dim=0)\n",
    "        logvar = scatter_mean(logvar, batch, dim=0)\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Only using reparam trick for training.\n",
    "        if self.training:\n",
    "            return std * eps + mu, logvar, mu, p_x\n",
    "        else:\n",
    "            return mu, logvar, mu, p_x\n",
    "\n",
    "'''\n",
    "\n",
    "    Prediction Branch\n",
    "    \n",
    "    Input: Input size, sequence length, hidden size in LSTM and number of layers in LSTM.\n",
    "    Output: Prediction\n",
    "    \n",
    "\n",
    "'''\n",
    "class Predictor(torch.nn.Module):\n",
    "    def __init__(self, input_size, seq_len, hidden_size, n_layers):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.prev_hidden = None\n",
    "        self.bs          = cfg.batch_size\n",
    "        self.input_size  = input_size\n",
    "        self.seq_len     = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers    = n_layers\n",
    "        \n",
    "        # Model\n",
    "        self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, self.n_layers, dropout=cfg.dropout, batch_first=True)\n",
    "        self.lin1 = torch.nn.Linear(self.hidden_size, len(cfg.action_map))\n",
    "    \n",
    "    def forward(self, p_x):\n",
    "        \n",
    "        if self.prev_hidden is None:\n",
    "            self.prev_hidden = (torch.zeros(self.n_layers, self.bs, self.hidden_size).cuda(device),\n",
    "                                torch.zeros(self.n_layers, self.bs, self.hidden_size).cuda(device))\n",
    "\n",
    "        # Reshape data to proper shape\n",
    "        input_reshape = p_x.reshape( self.bs, self.seq_len, -1 ).to(device)\n",
    "        \n",
    "        # Fed LSTM\n",
    "        q, h = self.lstm( input_reshape , self.prev_hidden )\n",
    "        \n",
    "        # Repackage hidden layer to reduce memory overflow\n",
    "        self.prev_hidden = repackage_hidden(h)\n",
    "        \n",
    "        # Get the LAST output from lstm\n",
    "        out = self.lin1(q[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "'''\n",
    "\n",
    "    Graph Reconstruction Branch\n",
    "    \n",
    "'''\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Experiment with more or larger FCL.\n",
    "        self.fc1 = nn.Linear(cfg.decoder_in, 64)\n",
    "        self.fc2 = nn.Linear(64, max_num_nodes*max_num_nodes)\n",
    "    \n",
    "    def forward(self, z_x):\n",
    "\n",
    "        out = F.relu(self.fc1(z_x)) # FCL with ReLU\n",
    "        out = self.fc2(out)         # FCL to output size \n",
    "        out = torch.sigmoid(out)    # Sigmoid\n",
    "        \n",
    "        return out\n",
    "\n",
    "'''\n",
    "\n",
    "    djNetwork model.\n",
    "    \n",
    "                -> Action Prediction\n",
    "    Encoder ->  -> Decoder -> Graph Reconstruction\n",
    "    \n",
    "'''\n",
    "class djNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(djNet, self).__init__()\n",
    "        # djNet struture\n",
    "        self.encoder = Encoder()\n",
    "        self.predictor = Predictor(8, 8, 8, 4)\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, logvar, mu, p_x = self.encoder(x) # Encoder input\n",
    "        p_z = self.predictor(p_x)            # Prediction input\n",
    "        q_z = self.decoder(z)                # Decoder input\n",
    "        \n",
    "        return q_z, logvar, mu, z, p_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create model with device.\n",
    "'''\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = djNet().to(device)\n",
    "\n",
    "\n",
    "print(\"#################\")\n",
    "print(model)\n",
    "print(\"#################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER USED\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "print(\"Optimizer is set.\")\n",
    "print(\"------------------------\")\n",
    "print(optimizer)\n",
    "\n",
    "print(\"####################\")\n",
    "\n",
    "# If SummeryWriter for tensorboard is used.\n",
    "if cfg.summery_writer:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    print(\"SummeryWriter is ON.\")\n",
    "    writer = SummaryWriter(comment=\"ENTER SOME COMMENT ABOUT MODEL\")\n",
    "else:\n",
    "    print(\"SummerWriter is OFF.\")\n",
    "\n",
    "print(\"####################\")\n",
    "\n",
    "# Loss function\n",
    "ap_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_criterion(inputs, targets, logvar, mu, ap_inputs, ap_targets):\n",
    "    # Reconstruction loss\n",
    "    bce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"sum\")\n",
    "    \n",
    "    # Action prediction loss\n",
    "    ap_loss = ap_criterion(ap_inputs, ap_targets)\n",
    "\n",
    "    # Regularization term\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())    \n",
    "\n",
    "    return bce_loss + kl_loss, ap_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    \n",
    "    Train model with specfic loader.\n",
    "    \n",
    "    Input: loader and model\n",
    "    Output: Reconstruction Loss and Action Prediciton Loss\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def train(loader, model):\n",
    "    model.train()\n",
    "    \n",
    "    recon_loss_all = 0\n",
    "    ap_loss_all = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "        data = data.to(device) # data to device\n",
    "\n",
    "        y_hat, logvar, mu, _, y_ap = model(data) # input model\n",
    "        prediction = y_hat.view(cfg.batch_size, -1, max_num_nodes) # reshape to prediction\n",
    "\n",
    "        # Creating targets\n",
    "        target_adj = util.to_dense_adj_max_node(data.edge_index, data.x, data.edge_attr, data.batch, max_num_nodes).cuda(device) # Create target adj matrix\n",
    "        target = data.y.view(cfg.batch_size, -1) # Reshape target\n",
    "        y_ap_true = target.argmax(axis=1)        # Get the ground truth target\n",
    "\n",
    "        # Compute loss\n",
    "        recon_loss, ap_loss = loss_criterion(prediction, target_adj, logvar, mu, y_ap, y_ap_true)\n",
    "        \n",
    "        net_loss = recon_loss * 0.7 + ap_loss # Create a net loss with weights applied to recon_loss\n",
    "\n",
    "        # Compute gradients and updates weights.\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add loss to output\n",
    "        recon_loss_all += recon_loss.item()\n",
    "        ap_loss_all += ap_loss.item()\n",
    "    \n",
    "    return recon_loss_all/(len(loader)*cfg.batch_size), ap_loss_all/(len(loader)*cfg.batch_size)\n",
    "\n",
    "\n",
    "'''\n",
    "    \n",
    "    Test model with a specific loader.\n",
    "    \n",
    "    Input: loader and model\n",
    "    Output: Reconstruction Loss, Action Prediciton Loss, Accuracy\n",
    "    \n",
    "'''\n",
    "\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    \n",
    "    recon_loss_all = 0\n",
    "    ap_loss_all = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "        data = data.to(device) # data to device\n",
    "\n",
    "        y_hat, logvar, mu, _, y_ap = model(data) # input model\n",
    "        prediction = y_hat.view(cfg.batch_size, -1, max_num_nodes) # reshape prediciton\n",
    "\n",
    "        # Creating targets\n",
    "        target_adj = util.to_dense_adj_max_node(data.edge_index, data.x, data.edge_attr, data.batch, max_num_nodes).cuda(device) # Create target adj matrix\n",
    "        target = data.y.view(cfg.batch_size, -1) # Reshape target\n",
    "        y_ap_true = target.argmax(axis=1) # Get the ground truth target\n",
    "        \n",
    "        pred = y_ap.max(1)[1] # Get the predicted action\n",
    "\n",
    "        # Compute loss\n",
    "        recon_loss, ap_loss = loss_criterion(prediction, target_adj, logvar, mu, y_ap, y_ap_true)\n",
    "\n",
    "        recon_loss_all += recon_loss.item()\n",
    "        ap_loss_all += ap_loss.item()\n",
    "        correct += pred.eq(y_ap_true).sum().item() # Compare prediction with ground truth\n",
    "    \n",
    "    return recon_loss_all/(len(loader)*cfg.batch_size), ap_loss_all/(len(loader)*cfg.batch_size), correct/(len(loader)*cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    TRAINING\n",
    "\n",
    "'''\n",
    "\n",
    "for epoch in range(1, cfg.epochs):\n",
    "    \n",
    "    train_recon_loss, train_ap_loss = train(train_loader, model)\n",
    "    _, _, train_ap_acc = test(train_loader, model)\n",
    "    validation_recon_loss, validation_ap_loss, validation_ap_acc = test(valid_loader, model)    \n",
    "\n",
    "    \n",
    "    # Writes to tensorboard\n",
    "    if cfg.summery_writer:\n",
    "        writer.add_scalar('AP_Acc/train', train_ap_acc, epoch)\n",
    "        writer.add_scalar('AP_Acc/validation', validation_ap_acc, epoch)\n",
    "        \n",
    "        writer.add_scalar('Recon_Loss/train', train_recon_loss, epoch)\n",
    "        writer.add_scalar('Recon_Loss/validation', validation_recon_loss, epoch)\n",
    "\n",
    "        writer.add_scalar('AP_Loss/train', train_ap_loss, epoch)\n",
    "        writer.add_scalar('AP_Loss/validation', validation_ap_loss, epoch)\n",
    "\n",
    "    \n",
    "    print(\"Epoch {:02d}, [T] RLoss: {:.2f}, APLoss: {:.4f}, Acc: {:.2f}% [V] RLoss: {:.2f}, APLoss: {:.4f}, Acc: {:.2f}%\".format( epoch, \n",
    "                                                                           train_recon_loss,\n",
    "                                                                           train_ap_loss,\n",
    "                                                                           train_ap_acc*100,\n",
    "                                                                           validation_recon_loss,\n",
    "                                                                           validation_ap_loss,\n",
    "                                                                           validation_ap_acc*100,\n",
    "                                                                            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Test that returns graph reconstruction and action prediction\n",
    "#\n",
    "# Input: loader, model, max_num_nodes, device\n",
    "# Output: \n",
    "# cr_pred   - graph reconstruction prediction\n",
    "# cr_gt     - graph reconstruction ground truth\n",
    "# ap_pred   - action prediction\n",
    "# ap_gt     - action ground truth\n",
    "# node_list - correct number of nodes of graphs.\n",
    "cr_pred, cr_gt, ap_pred, ap_target, node_list = m.test(test_loader, model, max_num_nodes, device)\n",
    "\n",
    "cm = confusion_matrix(ap_target, ap_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(ap_target, ap_pred, target_names=cfg.action_map, labels=[i for i in range(len(cfg.action_map))]))\n",
    "\n",
    "util.calc_auc_roc(cr_gt, cr_pred, node_list, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=cfg.action_map, yticklabels=cfg.action_map, cmap=\"YlGnBu\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Normalized')\n",
    "plt.show(block=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=cfg.action_map, yticklabels=cfg.action_map, cmap=\"YlGnBu\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Number of predictions')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Example on how to save and load models\n",
    "\"\"\"\n",
    "\n",
    "_SAVE = False\n",
    "\n",
    "if _SAVE:\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': 0,\n",
    "                }, \"./MANIAC_final_models/MANIAC_final_4w_dim_64_increase_mu_64.pt\")\n",
    "    print(\"SAVED!\")\n",
    "else:\n",
    "    checkpoint = torch.load(\"./FINAL_RESULTS/2w/maniac_416.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"LOADED MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
