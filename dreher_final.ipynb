{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch_geometric as tg\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pathlib import PurePath\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "from xml.dom import minidom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy import argmax\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import Utils.utils as util\n",
    "import Utils.config as conf\n",
    "import Utils.bad_utils as b_utils\n",
    "\n",
    "from Utils import BAD as b\n",
    "\n",
    "# Load BAC config\n",
    "cfg = conf.get_bac_cfg()\n",
    "\n",
    "# Getting folders\n",
    "_FOLDER = os.getcwd() + \"/BAD/\" # Where processed data are located\n",
    "MAIN_PATH = \"/DREHER/bimacs_derived_data/\" # Where BAD json objects are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    This is used to create new datasets.\n",
    "    \n",
    "    Example:\n",
    "    train_set = BAD_DS( \"FOLDER_TO_BAD_JSON\", split_type)\n",
    "    \n",
    "    All settings is set inside config.py\n",
    "    except save new data and create new dataset\n",
    "\n",
    "'''\n",
    "_SAVE_RAW_DATA = False\n",
    "_CREATE_DATASET = False\n",
    "\n",
    "if _CREATE_DATASET:\n",
    "    train_set = b.BAD_DS(MAIN_PATH, \"training\")\n",
    "    val_set = b.BAD_DS(MAIN_PATH, \"validation\")\n",
    "    test_set = b.BAD_DS(MAIN_PATH, \"test\")\n",
    "\n",
    "\n",
    "# USED TO SAVE RAW TRAINING DATA\n",
    "if _SAVE_RAW_DATA:\n",
    "    with open(os.path.join(_FOLDER + \"raw/bad_training_\" + str(cfg.time_window) + \"w.pt\"), 'wb') as f:\n",
    "                torch.save(train_set, f)\n",
    "    with open(os.path.join(_FOLDER + \"raw/bad_validation_\" + str(cfg.time_window) + \"w.pt\"), 'wb') as f:\n",
    "                torch.save(val_set, f)\n",
    "    with open(os.path.join(_FOLDER + \"raw/bad_test_\" + str(cfg.time_window) + \"w.pt\"), 'wb') as f:\n",
    "                torch.save(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    This is used to create load MANIAC dataset into DataLoader.\n",
    "    \n",
    "    Example)\n",
    "    To load processed or create preprocessed data into list.\n",
    "    train_dataset = b.BadIMDS(_FOLDER, \"train\")\n",
    "    \n",
    "    Creates a DataLoader of the loaded dataset.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "'''\n",
    "\n",
    "# Loading pre-processed or creates new processed pt files into FOLDER/processed/\n",
    "train_dataset = b.BadIMDS(_FOLDER + \"/\", \"train\")\n",
    "val_dataset = b.BadIMDS(_FOLDER + \"/\", \"valid\")\n",
    "test_dataset = b.BadIMDS(_FOLDER + \"/\", \"test\")\n",
    "\n",
    "#####################PRINT################################\n",
    "print(\"Total graphs:\\t {}\\n=========\".format(len(train_dataset)+len(test_dataset)+len(val_dataset)))\n",
    "print(\"Training: \\t {}\".format(len(train_dataset)))\n",
    "print(\"Test: \\t\\t {}\".format(len(test_dataset)))\n",
    "print(\"Validation: \\t {}\\n=========\".format(len(val_dataset)))\n",
    "#####################PRINT################################\n",
    "\n",
    "# Create data loaders from dataset.\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.DataLoader\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "#####################PRINT################################\n",
    "print(\"Total batchs:\\t {}\\n=========\".format(len(train_loader)+len(test_loader)+len(valid_loader)))\n",
    "print(\"Training: \\t {}\".format(len(train_loader)))\n",
    "print(\"Test: \\t\\t {}\".format(len(test_loader)))\n",
    "print(\"Validation: \\t {}\\n=========\".format(len(valid_loader)))\n",
    "#####################PRINT################################\n",
    "\n",
    "# Get maximum node for graph reconstruction\n",
    "max_num_nodes_train = max([len(i.x) for i in train_dataset])\n",
    "max_num_nodes_valid = max([len(i.x) for i in val_dataset])\n",
    "max_num_nodes_test = max([len(i.x) for i in test_dataset])\n",
    "max_num_nodes = max(max_num_nodes_test, max_num_nodes_train, max_num_nodes_valid)\n",
    "\n",
    "#####################PRINT################################\n",
    "print(\"Max number of nodes found:\", max_num_nodes)\n",
    "#####################PRINT################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, ReLU, ELU\n",
    "from torch_geometric.nn import NNConv, BatchNorm\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "'''\n",
    "\n",
    "    Encoder with 2x NNConv, BN\n",
    "    \n",
    "    Outputs mu, log variance, mu, prediction\n",
    "\n",
    "'''\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # NN for NNConv features\n",
    "        nn = Sequential(Linear(len(cfg.spatial_map), 512), ReLU(), Linear(512, cfg.channels * cfg.channels * 2 ))\n",
    "        nn2 = Sequential(Linear(len(cfg.spatial_map), 512), ReLU(), Linear(512, cfg.decoder_in * cfg.channels * 2 ))\n",
    "        \n",
    "        # Encoder\n",
    "        self.lin   = torch.nn.Linear(len(cfg.objects), cfg.channels) # FCL\n",
    "        self.conv1 = NNConv(cfg.channels, cfg.channels*2, nn, aggr='mean')\n",
    "        self.bn1   = BatchNorm(cfg.channels*2)\n",
    "        self.conv2 = NNConv(cfg.channels*2, cfg.channels, nn, aggr='mean')\n",
    "        \n",
    "        # Z-representation as mu and log\n",
    "        self.mu = NNConv(cfg.channels*2, cfg.decoder_in, nn2, aggr='max')\n",
    "        self.logvar = NNConv(cfg.channels*2, cfg.decoder_in, nn2, aggr='max')\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Input\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # FCL\n",
    "        out = F.relu(self.lin(data.x))\n",
    "        \n",
    "        # Used for ACTION PREDICTION\n",
    "        hidden = F.relu(self.conv1(out, data.edge_index, data.edge_attr))       # Conv1\n",
    "        hidden = self.bn1(hidden)                                               # BatchNorm\n",
    "        conv2_out = F.relu(self.conv2(hidden, data.edge_index, data.edge_attr)) # Conv2\n",
    "        \n",
    "        p_x = scatter_mean(conv2_out, batch, dim=0)\n",
    "        \n",
    "        # Used for GRAPH RECONSTRUCTION\n",
    "        mu = self.mu(hidden, data.edge_index, data.edge_attr)\n",
    "        logvar = self.logvar(hidden, data.edge_index, data.edge_attr)\n",
    "        mu = scatter_mean(mu, batch, dim=0)\n",
    "        logvar = scatter_mean(logvar, batch, dim=0)\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Only using reparam trick for training.\n",
    "        if self.training:\n",
    "            return std * eps + mu, logvar, mu, p_x\n",
    "        else:\n",
    "            return mu, logvar, mu, p_x\n",
    "\n",
    "'''\n",
    "\n",
    "    Prediction Branch\n",
    "    \n",
    "    Input: Input size, sequence length, hidden size in LSTM and number of layers in LSTM.\n",
    "    Output: Prediction\n",
    "    \n",
    "\n",
    "'''\n",
    "class Predictor(torch.nn.Module):\n",
    "    def __init__(self, input_size, seq_len, hidden_size, n_layers):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.prev_hidden = None\n",
    "        self.bs          = cfg.batch_size\n",
    "        self.input_size  = input_size\n",
    "        self.seq_len     = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers    = n_layers\n",
    "        \n",
    "        # Model\n",
    "        self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, self.n_layers, dropout=cfg.dropout, batch_first=True)\n",
    "        self.lin1 = torch.nn.Linear(self.hidden_size, len(cfg.action_map))\n",
    "    \n",
    "    def forward(self, p_x):\n",
    "        \n",
    "        if self.prev_hidden is None:\n",
    "            self.prev_hidden = (torch.zeros(self.n_layers, self.bs, self.hidden_size).cuda(device),\n",
    "                                torch.zeros(self.n_layers, self.bs, self.hidden_size).cuda(device))\n",
    "\n",
    "        # Reshape data to proper shape\n",
    "        input_reshape = p_x.reshape( self.bs, self.seq_len, -1 ).to(device)\n",
    "        \n",
    "        # Fed LSTM\n",
    "        q, h = self.lstm( input_reshape , self.prev_hidden )\n",
    "        \n",
    "        # Repackage hidden layer to reduce memory overflow\n",
    "        self.prev_hidden = repackage_hidden(h)\n",
    "        \n",
    "        # Get the LAST output from lstm\n",
    "        out = self.lin1(q[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "'''\n",
    "\n",
    "    Graph Reconstruction Branch\n",
    "    \n",
    "'''\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Experiment with more or larger FCL.\n",
    "        self.fc1 = nn.Linear(cfg.decoder_in, 64)\n",
    "        self.fc2 = nn.Linear(64, max_num_nodes*max_num_nodes)\n",
    "    \n",
    "    def forward(self, z_x):\n",
    "\n",
    "        out = F.relu(self.fc1(z_x)) # FCL with ReLU\n",
    "        out = self.fc2(out)         # FCL to output size \n",
    "        out = torch.sigmoid(out)    # Sigmoid\n",
    "        \n",
    "        return out\n",
    "\n",
    "'''\n",
    "\n",
    "    djNetwork model.\n",
    "    \n",
    "                -> Action Prediction\n",
    "    Encoder ->  -> Decoder -> Graph Reconstruction\n",
    "    \n",
    "'''\n",
    "class djNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(djNet, self).__init__()\n",
    "        # djNet struture\n",
    "        self.encoder = Encoder()\n",
    "        self.predictor = Predictor(8, 8, 8, 2)\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, logvar, mu, p_x = self.encoder(x) # Encoder input\n",
    "        p_z = self.predictor(p_x)            # Prediction input\n",
    "        q_z = self.decoder(z)                # Decoder input\n",
    "        \n",
    "        return q_z, logvar, mu, z, p_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create model with device.\n",
    "'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = djNet().to(device)\n",
    "\n",
    "\n",
    "print(\"#################\")\n",
    "print(model)\n",
    "print(\"#################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER USED\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "print(\"Optimizer is set.\")\n",
    "print(\"------------------------\")\n",
    "print(optimizer)\n",
    "\n",
    "print(\"####################\")\n",
    "\n",
    "# If SummeryWriter for tensorboard is used.\n",
    "if cfg.summery_writer:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    print(\"SummeryWriter is ON.\")\n",
    "    writer = SummaryWriter(comment=\"ENTER SOME COMMENT ABOUT MODEL\")\n",
    "else:\n",
    "    print(\"SummerWriter is OFF.\")\n",
    "\n",
    "print(\"####################\")\n",
    "\n",
    "# Loss function\n",
    "ap_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_criterion(inputs, targets, logvar, mu, ap_inputs, ap_targets):\n",
    "    # Reconstruction loss\n",
    "    bce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"sum\")\n",
    "    \n",
    "    # Action prediction loss\n",
    "    ap_loss = ap_criterion(ap_inputs, ap_targets)\n",
    "\n",
    "    # Regularization term\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())    \n",
    "\n",
    "    return bce_loss + kl_loss, ap_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    \n",
    "    Train model with specfic loader.\n",
    "    \n",
    "    Input: loader and model\n",
    "    Output: Reconstruction Loss and Action Prediciton Loss\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def train(loader, model):\n",
    "    model.train()\n",
    "    \n",
    "    recon_loss_all = 0\n",
    "    ap_loss_all = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "\n",
    "        y_hat, logvar, mu, _, y_ap = model(data) # input model\n",
    "        prediction = y_hat.view(_bs, -1, max_num_nodes) # reshape to prediction\n",
    "\n",
    "        # Creating targets\n",
    "        target_adj = util.to_dense_adj_max_node(data.edge_index, data.x, data.edge_attr, data.batch, max_num_nodes)\n",
    "        target = data.y.view(_bs, -1) # reshape target\n",
    "        y_ap_true = target.argmax(axis=1) # get the ground truth target\n",
    "\n",
    "        # Compute loss\n",
    "        recon_loss, ap_loss = loss_criterion(prediction, target_adj, logvar, mu, y_ap, y_ap_true)\n",
    "        net_loss = recon_loss * 0.6 + ap_loss\n",
    "\n",
    "        # Compute gradients and updates weights.\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        recon_loss_all += recon_loss.item()\n",
    "        ap_loss_all += ap_loss.item()\n",
    "    \n",
    "    return recon_loss_all/(len(loader)*_bs), ap_loss_all/(len(loader)*_bs)\n",
    "\n",
    "\n",
    "'''\n",
    "    \n",
    "    Test model with a specific loader.\n",
    "    \n",
    "    Input: loader and model\n",
    "    Output: Reconstruction Loss, Action Prediciton Loss, Accuracy\n",
    "    \n",
    "'''\n",
    "\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    \n",
    "    recon_loss_all = 0\n",
    "    ap_loss_all = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "\n",
    "        y_hat, logvar, mu, _, y_ap = model(data)\n",
    "        prediction = y_hat.view(_bs, -1, max_num_nodes)\n",
    "\n",
    "        # Creating targets\n",
    "        target_adj = util.to_dense_adj_max_node(data.edge_index, data.x, data.edge_attr, data.batch, max_num_nodes)\n",
    "        target = data.y.view(_bs, -1)\n",
    "        y_ap_true = target.argmax(axis=1)\n",
    "        \n",
    "        pred = y_ap.max(1)[1]\n",
    "\n",
    "        # Compute loss\n",
    "        recon_loss, ap_loss = loss_criterion(prediction, target_adj, logvar, mu, y_ap, y_ap_true)\n",
    "\n",
    "        recon_loss_all += recon_loss.item()\n",
    "        ap_loss_all += ap_loss.item()\n",
    "        correct += pred.eq(y_ap_true).sum().item()\n",
    "    \n",
    "    return recon_loss_all/(len(loader)*_bs), ap_loss_all/(len(loader)*_bs), correct/(len(loader)*_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    TRAINING\n",
    "\n",
    "'''\n",
    "\n",
    "for epoch in range(1, cfg.epochs):\n",
    "    \n",
    "    train_recon_loss, train_ap_loss = train(train_loader, model)\n",
    "    _, _, train_ap_acc = test(train_loader, model)\n",
    "    validation_recon_loss, validation_ap_loss, validation_ap_acc = test(valid_loader, model)\n",
    "\n",
    "    # Writes to tensorboard\n",
    "    if cfg.summery_writer:\n",
    "        writer.add_scalar('AP_Acc/train', train_ap_acc, epoch)\n",
    "        writer.add_scalar('AP_Acc/validation', validation_ap_acc, epoch)\n",
    "\n",
    "        writer.add_scalar('Recon_Loss/train', train_recon_loss, epoch)\n",
    "        writer.add_scalar('Recon_Loss/validation', validation_recon_loss, epoch)\n",
    "\n",
    "        writer.add_scalar('AP_Loss/train', train_ap_loss, epoch)\n",
    "        writer.add_scalar('AP_Loss/validation', validation_ap_loss, epoch)\n",
    "\n",
    "    \n",
    "    print(\"Epoch {:02d}, [T] RLoss: {:.2f}, APLoss: {:.4f}, Acc: {:.2f}% [V] RLoss: {:.2f}, APLoss: {:.4f}, Acc: {:.2f}%, TAcc: {:.2f}\".format( epoch, \n",
    "                                                                           train_recon_loss,\n",
    "                                                                           train_ap_loss,\n",
    "                                                                           train_ap_acc*100,\n",
    "                                                                           validation_recon_loss,\n",
    "                                                                           validation_ap_loss,\n",
    "                                                                           validation_ap_acc*100,\n",
    "                                                                           test_ap_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SAVE = False\n",
    "if _SAVE:\n",
    "    torch.save({\n",
    "                'epoch': 99,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': 4.110431,\n",
    "                }, \"./MANIAC_final_models/MANIAC_final_4w_dim_64_increase_mu_64.pt\")\n",
    "    print(\"SAVED!\")\n",
    "else:\n",
    "    checkpoint = torch.load(\"/data/tmp/dj_data/runs/dreher_10.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"LOADED MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Test that returns graph reconstruction and action prediction\n",
    "#\n",
    "# Input: loader, model, max_num_nodes, device\n",
    "# Output: \n",
    "# cr_pred   - graph reconstruction prediction\n",
    "# cr_gt     - graph reconstruction ground truth\n",
    "# ap_pred   - action prediction\n",
    "# ap_gt     - action ground truth\n",
    "# top3      - list of top 3 predicitons\n",
    "# node_list - correct number of nodes of graphs.\n",
    "cr_pred, cr_gt, ap_pred, ap_target, top3, node_list = b.test(test_loader, model, max_num_nodes, device)\n",
    "\n",
    "cm = confusion_matrix(ap_target, ap_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(ap_target, ap_pred, target_names=cfg.action_map, labels=[i for i in range(len(cfg.action_map))]))\n",
    "\n",
    "util.calc_auc_roc(cr_gt, cr_pred, node_list, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = []\n",
    "for i in range(len(top3)):\n",
    "    if action_map[ap_target[i].astype(int)] in top3[i]:\n",
    "        new_pred.append(ap_target[i])\n",
    "    else:\n",
    "        new_pred.append(action_map.index(top3[i][0]))\n",
    "\n",
    "print(\"top 3\")\n",
    "\n",
    "print(classification_report(ap_target.astype(int), new_pred, target_names=action_map, labels=[0,1,2,3,4,5,6,7, 8, 9, 10, 11, 12,13,14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=action_map, yticklabels=action_map, cmap=\"YlGnBu\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Normalized')\n",
    "plt.show(block=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=action_map, yticklabels=action_map, cmap=\"YlGnBu\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Number of predictions')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Example on how to save and load models\n",
    "\"\"\"\n",
    "\n",
    "_SAVE = False\n",
    "\n",
    "if _SAVE:\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': 0,\n",
    "                }, \"./BAD_MODEL/bad_84.pt\")\n",
    "    print(\"SAVED!\")\n",
    "else:\n",
    "    checkpoint = torch.load(\"./BAD_MODEL/bad_84.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"LOADED MODEL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
